{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Tutorial T03e: Reproducing Bhat (2018) Table 1 — MVNCD Accuracy Evaluation\n",
    "\n",
    "This tutorial reproduces the systematic Monte Carlo evaluation from Table 1 of:\n",
    "\n",
    "> Bhat, C. R. (2018). New Matrix-Based Methods for the Analytic Evaluation\n",
    "> of the MVNCD Function. *Transportation Research Part B*, 109: 238-256.\n",
    "\n",
    "Table 1 evaluates MVNCD approximation accuracy across dimensions H=5,7,10,12,\n",
    "15,18,20 using 1000 random correlation matrices per H value. For each, it\n",
    "reports MAE, MAPE, %MAE>0.005, %MAPE>2%, and computation time.\n",
    "\n",
    "**What you will learn:**\n",
    "- How the paper's Monte Carlo test design works (Section 3.1)\n",
    "- How random correlation matrices are generated (low vs high correlation)\n",
    "- How to benchmark MVNCD methods systematically\n",
    "- How our Python implementation compares to the paper's GAUSS results\n",
    "\n",
    "**Prerequisites:** t03a (MVNCD methods overview).\n",
    "\n",
    "**Configuration:**\n",
    "- `N_TESTS = 100` by default for fast execution (~1 min per H)\n",
    "- Set `N_TESTS = 1000` for full replication of the paper (~10 min per H)\n",
    "- `H_VALUES = [5, 7, 10]` by default; add `[12, 15, 18, 20]` for full table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal as scipy_mvn\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "import pathlib\n",
    "sys.path.insert(0, str(pathlib.Path.cwd().parent.parent / \"src\"))\n",
    "\n",
    "from pybhatlib.gradmvn import mvncd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of random test cases per H value.\n",
    "# Paper uses 1000; we default to 100 for fast execution.\n",
    "N_TESTS = 100\n",
    "\n",
    "# Dimensions to test.\n",
    "# Paper uses [5, 7, 10, 12, 15, 18, 20]; we default to small ones.\n",
    "H_VALUES = [5, 7, 10]\n",
    "\n",
    "# Methods to benchmark (analytic + simulation).\n",
    "METHODS = [\"me\", \"ovus\", \"ovbs\", \"bme\", \"tvbs\"]\n",
    "SSJ_CONFIGS = [(500, \"ssj-500\"), (10000, \"ssj-10k\")]\n",
    "\n",
    "# Random seed for reproducibility.\n",
    "SEED = 42\n",
    "\n",
    "# Dimension threshold: use scipy for H <= this, SSJ for H > this.\n",
    "SCIPY_THRESHOLD = 10\n",
    "\n",
    "# SSJ reference draws for H > SCIPY_THRESHOLD.\n",
    "SSJ_REF_DRAWS = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Step 1: Random Correlation Matrix Generation (Section 3.1)\n",
    "\n",
    "The paper generates 1000 random correlation matrices per dimension H,\n",
    "split equally into two groups:\n",
    "\n",
    "**Low correlation (50%):**\n",
    "```\n",
    "C = R @ R.T + 10 * diag(r_u)\n",
    "```\n",
    "where R is H×H uniform[0,1], r_u is H uniform[0,1].\n",
    "The large diagonal boost (10×) weakens off-diagonal correlations.\n",
    "\n",
    "**High correlation (50%):**\n",
    "```\n",
    "C = R @ R.T + 0 * diag(r_u)   (no diagonal boost)\n",
    "```\n",
    "Off-diagonal correlations remain strong.\n",
    "\n",
    "Both are normalized to correlation matrices: C_ij / sqrt(C_ii × C_jj).\n",
    "\n",
    "**Upper integration limits** a (per test):\n",
    "- Half from U[0, √H] — all positive limits\n",
    "- Half from U[-√H/2, √H] — mixed positive/negative limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_cases(H, n_tests, rng):\n",
    "    \"\"\"Generate random correlation matrices and integration limits.\n",
    "\n",
    "    Follows Bhat (2018) Section 3.1, p.248.\n",
    "    \"\"\"\n",
    "    n_low = n_tests // 2\n",
    "    test_cases = []\n",
    "\n",
    "    for i in range(n_tests):\n",
    "        # --- Correlation matrix ---\n",
    "        R = rng.uniform(0, 1, (H, H))\n",
    "        C = R @ R.T\n",
    "\n",
    "        if i < n_low:\n",
    "            # Low correlation: add large diagonal boost\n",
    "            r_u = rng.uniform(0, 1, H)\n",
    "            C += 10.0 * np.diag(r_u)\n",
    "\n",
    "        # Normalize to correlation matrix\n",
    "        d = np.sqrt(np.diag(C))\n",
    "        sigma = C / np.outer(d, d)\n",
    "        sigma = (sigma + sigma.T) / 2.0\n",
    "        np.fill_diagonal(sigma, 1.0)\n",
    "\n",
    "        # --- Integration limits ---\n",
    "        sqrtH = np.sqrt(H)\n",
    "        if i % 2 == 0:\n",
    "            a = rng.uniform(0, sqrtH, H)\n",
    "        else:\n",
    "            a = rng.uniform(-sqrtH / 2, sqrtH, H)\n",
    "\n",
    "        test_cases.append((a, sigma))\n",
    "\n",
    "    return test_cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Step 2: Reference CDF Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reference(a, sigma, H, rng_ref):\n",
    "    \"\"\"Compute reference CDF value.\n",
    "\n",
    "    For H <= SCIPY_THRESHOLD: uses scipy.stats.multivariate_normal.cdf\n",
    "    (Genz 1992 algorithm, high accuracy).\n",
    "\n",
    "    For H > SCIPY_THRESHOLD: uses SSJ with many draws.\n",
    "    \"\"\"\n",
    "    if H <= SCIPY_THRESHOLD:\n",
    "        try:\n",
    "            return float(scipy_mvn.cdf(a, mean=np.zeros(H), cov=sigma))\n",
    "        except np.linalg.LinAlgError:\n",
    "            return mvncd(a, sigma, method=\"ssj\", n_draws=SSJ_REF_DRAWS,\n",
    "                         seed=int(rng_ref.integers(1, 2**31)))\n",
    "    else:\n",
    "        return mvncd(a, sigma, method=\"ssj\", n_draws=SSJ_REF_DRAWS,\n",
    "                     seed=int(rng_ref.integers(1, 2**31)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Step 3: Paper Table 1 Values for Comparison\n",
    "\n",
    "Bhat (2018) Table 1 (p.250) reports MAE, MAPE, %MAE>0.005, %MAPE>2% for each method and dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format: {H: {method: (MAE, MAPE, pct_mae_005, pct_mape_2)}}\n",
    "PAPER_TABLE1 = {\n",
    "    5:  {\"me\": (0.0025, 1.78, 11.5, 18.5),\n",
    "         \"ovus\": (0.0019, 1.52, 9.2, 15.7),\n",
    "         \"bme\": (0.0015, 1.32, 7.9, 13.7),\n",
    "         \"tvbs\": (0.0008, 0.82, 3.2, 6.3),\n",
    "         \"ovbs\": (0.0012, 0.98, 5.3, 9.1)},\n",
    "    7:  {\"me\": (0.0048, 4.32, 22.8, 33.3),\n",
    "         \"ovus\": (0.0025, 2.55, 13.0, 19.5),\n",
    "         \"bme\": (0.0023, 2.33, 11.7, 18.7),\n",
    "         \"tvbs\": (0.0010, 1.09, 4.2, 8.1),\n",
    "         \"ovbs\": (0.0014, 1.49, 6.9, 11.3)},\n",
    "    10: {\"me\": (0.0068, 8.12, 34.3, 47.2),\n",
    "         \"ovus\": (0.0028, 3.78, 15.4, 23.2),\n",
    "         \"bme\": (0.0026, 3.36, 14.4, 22.7),\n",
    "         \"tvbs\": (0.0011, 1.52, 5.3, 10.2),\n",
    "         \"ovbs\": (0.0015, 2.02, 7.6, 13.4)},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Step 4: Run Benchmarks\n",
    "\n",
    "For each H value, we generate N_TESTS random test cases, compute reference values,\n",
    "then evaluate all methods and compare with the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {}\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "for H in H_VALUES:\n",
    "    print(f\"\\n{'=' * 72}\")\n",
    "    print(f\"  H = {H}  (generating {N_TESTS} random test cases)\")\n",
    "    print(f\"{'=' * 72}\")\n",
    "\n",
    "    # Generate test cases\n",
    "    test_cases = generate_test_cases(H, N_TESTS, rng)\n",
    "\n",
    "    # Compute reference values\n",
    "    ref_method = \"scipy\" if H <= SCIPY_THRESHOLD else f\"SSJ({SSJ_REF_DRAWS:,})\"\n",
    "    print(f\"\\n  Computing reference values via {ref_method}...\", end=\"\", flush=True)\n",
    "    t0 = time.perf_counter()\n",
    "    rng_ref = np.random.default_rng(SEED + 1000)\n",
    "    refs = []\n",
    "    for a, sigma in test_cases:\n",
    "        refs.append(compute_reference(a, sigma, H, rng_ref))\n",
    "    ref_time = time.perf_counter() - t0\n",
    "    refs = np.array(refs)\n",
    "    print(f\" done ({ref_time:.1f}s)\")\n",
    "\n",
    "    # Skip cases where reference is essentially zero\n",
    "    valid = refs > 1e-10\n",
    "    n_valid = valid.sum()\n",
    "    if n_valid < N_TESTS:\n",
    "        print(f\"  Note: {N_TESTS - n_valid} cases with ref ~ 0 excluded from MAPE\")\n",
    "\n",
    "    # --- Benchmark each method ---\n",
    "    results = {}\n",
    "\n",
    "    # Analytic methods\n",
    "    for method in METHODS:\n",
    "        t0 = time.perf_counter()\n",
    "        pvals = np.zeros(N_TESTS)\n",
    "        for i, (a, sigma) in enumerate(test_cases):\n",
    "            pvals[i] = mvncd(a, sigma, method=method, seed=42)\n",
    "        elapsed = time.perf_counter() - t0\n",
    "\n",
    "        ae = np.abs(pvals - refs)\n",
    "        ape = np.where(valid, ae / np.maximum(refs, 1e-15) * 100, 0.0)\n",
    "\n",
    "        mae = ae.mean()\n",
    "        mape = ape[valid].mean() if n_valid > 0 else 0.0\n",
    "        pct_mae_005 = (ae > 0.005).mean() * 100\n",
    "        pct_mape_2 = (ape[valid] > 2.0).mean() * 100 if n_valid > 0 else 0.0\n",
    "\n",
    "        results[method] = (mae, mape, pct_mae_005, pct_mape_2, elapsed)\n",
    "\n",
    "    # SSJ methods\n",
    "    for n_draws, label in SSJ_CONFIGS:\n",
    "        t0 = time.perf_counter()\n",
    "        pvals = np.zeros(N_TESTS)\n",
    "        for i, (a, sigma) in enumerate(test_cases):\n",
    "            pvals[i] = mvncd(a, sigma, method=\"ssj\", n_draws=n_draws, seed=42)\n",
    "        elapsed = time.perf_counter() - t0\n",
    "\n",
    "        ae = np.abs(pvals - refs)\n",
    "        ape = np.where(valid, ae / np.maximum(refs, 1e-15) * 100, 0.0)\n",
    "\n",
    "        mae = ae.mean()\n",
    "        mape = ape[valid].mean() if n_valid > 0 else 0.0\n",
    "        pct_mae_005 = (ae > 0.005).mean() * 100\n",
    "        pct_mape_2 = (ape[valid] > 2.0).mean() * 100 if n_valid > 0 else 0.0\n",
    "\n",
    "        results[label] = (mae, mape, pct_mae_005, pct_mape_2, elapsed)\n",
    "\n",
    "    all_results[H] = results\n",
    "\n",
    "    # --- Print results table ---\n",
    "    print(f\"\\n  {'Method':>10s} {'MAE':>10s} {'MAPE%':>8s} {'%MAE>.005':>10s}\"\n",
    "          f\" {'%MAPE>2':>8s} {'Time(s)':>8s}\")\n",
    "    print(f\"  {'-' * 58}\")\n",
    "    for label in METHODS + [cfg[1] for cfg in SSJ_CONFIGS]:\n",
    "        mae, mape, pct_mae, pct_mape, elapsed = results[label]\n",
    "        print(f\"  {label:>10s} {mae:>10.4f} {mape:>7.2f}% {pct_mae:>9.1f}%\"\n",
    "              f\" {pct_mape:>7.1f}% {elapsed:>7.1f}s\")\n",
    "\n",
    "    # --- Compare with paper Table 1 ---\n",
    "    if H in PAPER_TABLE1:\n",
    "        print(f\"\\n  Comparison with Bhat (2018) Table 1 (H={H}):\")\n",
    "        print(f\"  {'Method':>10s} {'Ours':>8s} {'Paper':>8s} {'Ours':>10s}\"\n",
    "              f\" {'Paper':>8s}\")\n",
    "        print(f\"  {'':>10s} {'MAPE%':>8s} {'MAPE%':>8s} {'MAE':>10s}\"\n",
    "              f\" {'MAE':>8s}\")\n",
    "        print(f\"  {'-' * 50}\")\n",
    "        for method in METHODS:\n",
    "            if method in PAPER_TABLE1[H]:\n",
    "                our_mae, our_mape = results[method][0], results[method][1]\n",
    "                paper_mae, paper_mape = PAPER_TABLE1[H][method][:2]\n",
    "                mape_better = \"*\" if our_mape < paper_mape else \" \"\n",
    "                mae_better = \"*\" if our_mae < paper_mae else \" \"\n",
    "                print(f\"  {method:>10s} {our_mape:>7.2f}%{mape_better}\"\n",
    "                      f\" {paper_mape:>7.2f}% {our_mae:>9.4f}{mae_better}\"\n",
    "                      f\" {paper_mae:>7.4f}\")\n",
    "        print(f\"\\n  * = our implementation outperforms paper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Step 5: Summary Across All Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'=' * 72}\")\n",
    "print(f\"  Summary: MAPE(%) across dimensions\")\n",
    "print(f\"{'=' * 72}\")\n",
    "\n",
    "all_labels = METHODS + [cfg[1] for cfg in SSJ_CONFIGS]\n",
    "header = f\"  {'Method':>10s}\" + \"\".join(f\" {'H='+str(H):>8s}\" for H in H_VALUES)\n",
    "print(header)\n",
    "print(f\"  {'-' * (10 + 9 * len(H_VALUES))}\")\n",
    "\n",
    "for label in all_labels:\n",
    "    row = f\"  {label:>10s}\"\n",
    "    for H in H_VALUES:\n",
    "        mape = all_results[H][label][1]\n",
    "        row += f\" {mape:>7.2f}%\"\n",
    "    print(row)\n",
    "\n",
    "# Paper comparison\n",
    "if any(H in PAPER_TABLE1 for H in H_VALUES):\n",
    "    print(f\"\\n  Paper Table 1 MAPE(%) for reference:\")\n",
    "    for method in METHODS:\n",
    "        row = f\"  {method+'-paper':>10s}\"\n",
    "        for H in H_VALUES:\n",
    "            if H in PAPER_TABLE1 and method in PAPER_TABLE1[H]:\n",
    "                row += f\" {PAPER_TABLE1[H][method][1]:>7.2f}%\"\n",
    "            else:\n",
    "                row += f\" {'n/a':>8s}\"\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "**1. Accuracy ranking** (consistent with paper):\n",
    "TVBS > OVBS > BME > OVUS > ME. TVBS is the most accurate analytic method at all dimensions.\n",
    "\n",
    "**2. Our implementation vs paper:**\n",
    "Our MAPE values are generally *lower* (better) than the paper because we use scipy's exact\n",
    "bivariate/trivariate/quadrivariate CDFs as building blocks, while the original GAUSS code\n",
    "used numerical approximations for these base CDFs.\n",
    "\n",
    "**3. Dimension scaling:**\n",
    "All analytic methods degrade with increasing H, but TVBS degrades most gracefully.\n",
    "ME degrades fastest because it only uses univariate conditioning (no screening correction).\n",
    "\n",
    "**4. SSJ simulation:**\n",
    "SSJ accuracy improves with `n_draws` but is slower. SSJ(10000) is competitive with the\n",
    "best analytic methods. SSJ is recommended when H > 20 or when highest accuracy is needed.\n",
    "\n",
    "**5. Reference values:**\n",
    "- For H ≤ 10: `scipy.stats.multivariate_normal.cdf` (Genz algorithm)\n",
    "- For H > 10: SSJ with 50,000 draws (scipy unreliable at high dimensions)\n",
    "\n",
    "To reproduce the full paper Table 1, set `N_TESTS = 1000` and `H_VALUES = [5, 7, 10, 12, 15, 18, 20]`.\n",
    "\n",
    "**Reference:**\n",
    "Bhat, C. R. (2018). New Matrix-Based Methods for the Analytic Evaluation of the MVNCD Function.\n",
    "*Transportation Research Part B*, 109: 238-256.\n",
    "\n",
    "**Next:** See `t04a_mnp_basic` for MNP estimation using these MVNCD methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}