{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial T03b: MVNCD Gradients.\n",
    "\n",
    "For gradient-based MNP estimation, we need not just P(X <= b) but also\nits partial derivatives with respect to the integration limits (b) and\nthe covariance matrix (sigma).\n\nWhat you will learn:\n  - mvncd_grad: simultaneous computation of MVNCD and its gradients\n  - Interpreting grad_a (sensitivity to integration limits)\n  - Interpreting grad_sigma (sensitivity to covariance)\n  - Numerical verification via finite differences\n\nPrerequisites: t03a (MVNCD methods).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os, sys\nimport numpy as np\nnp.set_printoptions(precision=4, suppress=True)\nimport pathlib\nsys.path.insert(0, str(pathlib.Path.cwd().parent.parent / \"src\"))\n\nfrom pybhatlib.gradmvn import mvncd, mvncd_grad\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: MVNCD Gradients for K=3\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sigma = np.array([\n    [1.0, 0.3, 0.1],\n    [0.3, 1.0, 0.4],\n    [0.1, 0.4, 1.0],\n])\na = np.array([1.0, 0.5, 0.0])\n\nresult = mvncd_grad(a, sigma)\n\nprint(f\"\\n  a (integration limits) = {a}\")\nprint(f\"  sigma =\\n{sigma}\")\nprint(f\"\\n  P(X <= a) = {result.prob:.6f}\")\nprint(f\"  grad_a = {result.grad_a}\")\nprint(f\"  grad_sigma shape = {result.grad_sigma.shape}\")\nprint(f\"  grad_sigma =\\n{result.grad_sigma}\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Interpreting grad_a\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(f\"\\n  grad_a[k] = dP/da_k = how P changes when limit k increases\")\nprint(f\"\\n  grad_a = {result.grad_a}\")\nprint(f\"\\n  All positive (as expected): increasing any limit increases P(X <= a)\")\n\nfor k in range(3):\n    print(f\"    dP/da_{k+1} = {result.grad_a[k]:.6f}\"\n          f\" — raising a_{k+1} from {a[k]:.1f} increases probability\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Verify grad_a (Finite Differences)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "eps = 1e-5\ngrad_a_fd = np.zeros(3)\n\nfor k in range(3):\n    a_plus = a.copy(); a_plus[k] += eps\n    a_minus = a.copy(); a_minus[k] -= eps\n    p_plus = mvncd(a_plus, sigma, method=\"scipy\")\n    p_minus = mvncd(a_minus, sigma, method=\"scipy\")\n    grad_a_fd[k] = (p_plus - p_minus) / (2 * eps)\n\nprint(f\"\\n  {'k':>4s} {'analytic':>12s} {'numerical':>12s} {'error':>12s}\")\nprint(f\"  {'-'*42}\")\nfor k in range(3):\n    err = abs(result.grad_a[k] - grad_a_fd[k])\n    print(f\"  {k+1:>4d} {result.grad_a[k]:>12.6f} {grad_a_fd[k]:>12.6f} {err:>12.2e}\")\n\nmax_err_a = np.max(np.abs(result.grad_a - grad_a_fd))\nprint(f\"\\n  Max error: {max_err_a:.2e}\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Verify grad_sigma (Finite Differences)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# grad_sigma is a vecdup vector (upper-tri elements, row-by-row)\nfrom pybhatlib.vecup import vecdup as _vecdup\nK_dim = 3\nn_upper = K_dim * (K_dim + 1) // 2\ngrad_sigma_fd = np.zeros(n_upper)\n\nidx = 0\nfor i in range(K_dim):\n    for j in range(i, K_dim):\n        sigma_plus = sigma.copy()\n        sigma_plus[i, j] += eps\n        sigma_plus[j, i] += eps  # keep symmetric\n        p_plus = mvncd(a, sigma_plus, method=\"scipy\")\n\n        sigma_minus = sigma.copy()\n        sigma_minus[i, j] -= eps\n        sigma_minus[j, i] -= eps\n        p_minus = mvncd(a, sigma_minus, method=\"scipy\")\n\n        grad_sigma_fd[idx] = (p_plus - p_minus) / (2 * eps)\n        idx += 1\n\nprint(f\"\\n  Analytic grad_sigma (vecdup): {result.grad_sigma}\")\nprint(f\"  Numerical grad_sigma:         {grad_sigma_fd}\")\n\nmax_err_s = np.max(np.abs(result.grad_sigma - grad_sigma_fd))\nprint(f\"\\n  Max error: {max_err_s:.2e}\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Connection to MNP Log-Likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"\"\"\n  In MNP estimation, the log-likelihood for observation q choosing\n  alternative i is:\n\n    log P_qi = log P(V_qi - V_qj > eps_qj - eps_qi, for all j != i)\n             = log MVNCD(a_qi, Lambda_qi)\n\n  where a_qi depends on beta (utility parameters) and Lambda_qi depends\n  on the error covariance structure.\n\n  The gradient of the log-likelihood is:\n    d(log P_qi)/d(beta) uses grad_a (through da/dbeta = X differences)\n    d(log P_qi)/d(Lambda) uses grad_sigma (through chain rules from t02c)\n\n  These gradients are computed analytically by mvncd_grad, avoiding the\n  need for costly numerical differentiation during optimization.\n\"\"\")\n\nprint(f\"  Next: t03c_mvncd_rect.py — Rectangular MVNCD for ordered probit\")\n"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}