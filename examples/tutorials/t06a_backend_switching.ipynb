{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial T06a: Backend Switching (NumPy vs PyTorch).\n",
    "\n",
    "pybhatlib uses a backend abstraction that lets the same code run on\nNumPy (CPU) or PyTorch (CPU/GPU). All numerical functions accept an\noptional `xp` parameter to select the backend.\n\nWhat you will learn:\n  - get_backend: obtain the current backend module\n  - set_backend: change the global default\n  - array_namespace: auto-detect backend from array type\n  - The xp parameter pattern used throughout pybhatlib\n  - When PyTorch is useful (autograd, GPU)\n\nPrerequisites: None.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os, sys\nimport numpy as np\nnp.set_printoptions(precision=4, suppress=True)\nimport pathlib\nsys.path.insert(0, str(pathlib.Path.cwd().parent.parent / \"src\"))\n\nfrom pybhatlib.backend import get_backend, set_backend, array_namespace\nfrom pybhatlib.vecup import vecdup, matdupfull\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Default Backend is NumPy\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "xp = get_backend()\nprint(f\"\\n  get_backend() -> {type(xp).__name__}\")\nprint(f\"  This is the standard NumPy module.\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The xp Parameter Pattern\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "A = np.array([[1, 2, 3], [2, 4, 5], [3, 5, 6]], dtype=float)\n\n# Explicit xp=numpy\nv = vecdup(A, xp=xp)\nprint(f\"\\n  vecdup(A, xp=numpy) = {v}\")\nprint(f\"  Type: {type(v)}\")\n\n# Without xp (uses default)\nv2 = vecdup(A)\nprint(f\"\\n  vecdup(A) = {v2}  (same result, default backend)\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: array_namespace — Auto-Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "arr_np = np.array([1.0, 2.0, 3.0])\ndetected = array_namespace(arr_np)\nprint(f\"\\n  Input: numpy array\")\nprint(f\"  array_namespace(arr) -> {type(detected).__name__}\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: PyTorch Backend\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "try:\n    import torch\n    torch_available = True\nexcept ImportError:\n    torch_available = False\n\nif torch_available:\n    xp_torch = get_backend(\"torch\")\n    print(f\"\\n  PyTorch is available: {torch.__version__}\")\n    print(f\"  get_backend('torch') -> {type(xp_torch).__name__}\")\n\n    # Same computation on both backends\n    A_np = np.array([[4, 2], [2, 5]], dtype=np.float64)\n    A_torch = torch.tensor([[4, 2], [2, 5]], dtype=torch.float64)\n\n    v_np = vecdup(A_np, xp=get_backend(\"numpy\"))\n    v_torch = vecdup(A_torch, xp=xp_torch)\n\n    print(f\"\\n  NumPy result:   {v_np}\")\n    print(f\"  PyTorch result: {v_torch.numpy()}\")\n    print(f\"  Match: {np.allclose(v_np, v_torch.numpy())}\")\n\n    # Roundtrip\n    M_np = matdupfull(v_np, xp=get_backend(\"numpy\"))\n    M_torch = matdupfull(v_torch, xp=xp_torch)\n    print(f\"\\n  NumPy matdupfull:\\n{M_np}\")\n    print(f\"  PyTorch matdupfull:\\n{M_torch.numpy()}\")\n    print(f\"  Match: {np.allclose(M_np, M_torch.numpy())}\")\n\n    # Auto-detect\n    detected_torch = array_namespace(A_torch)\n    print(f\"\\n  array_namespace(torch tensor) -> {type(detected_torch).__name__}\")\nelse:\n    print(f\"\\n  PyTorch is not installed.\")\n    print(f\"  Install with: pip install pybhatlib[torch]\")\n    print(f\"  All tutorials work fine with NumPy only.\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: When PyTorch Is Useful\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"\"\"\n  NumPy (default):\n  - CPU-only, always available\n  - All pybhatlib features supported\n  - Best for most use cases\n\n  PyTorch:\n  - Automatic differentiation (autograd) for gradient computation\n  - GPU acceleration for large-scale problems\n  - Useful for:\n    * Very large datasets where GPU parallelism helps\n    * Research on new gradient methods\n    * Integration with PyTorch-based ML pipelines\n\n  The xp pattern means you write code once and it works on both:\n\n    def my_function(A, xp=None):\n        if xp is None:\n            xp = get_backend()\n        # Use xp.array(), xp.zeros(), etc.\n        return xp.sum(A)\n\"\"\")\n\nprint(f\"  Next: t06b_custom_specs.py — Custom model specifications\")\n"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}