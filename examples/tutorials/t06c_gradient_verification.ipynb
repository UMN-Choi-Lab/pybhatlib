{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial T06c: Gradient Verification via Finite Differences.\n",
    "\n",
    "Correct gradients are critical for optimization convergence.\nThis capstone tutorial shows the systematic pattern for verifying\nany analytic gradient against numerical finite differences.\n\nWhat you will learn:\n  - Central finite differences: formula, eps selection\n  - Verifying vecup gradients (det of matdupfull)\n  - Verifying matgradient (gradcovcor)\n  - Verifying gradmvn (mvncd_grad)\n  - A reusable verify_gradient() template function\n  - Best practices: eps, relative vs absolute tolerance\n\nPrerequisites: t01a, t02a, t03b.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os, sys\nimport numpy as np\nnp.set_printoptions(precision=4, suppress=True)\nimport pathlib\nsys.path.insert(0, str(pathlib.Path.cwd().parent.parent / \"src\"))\n\nfrom pybhatlib.vecup import vecdup, matdupfull\nfrom pybhatlib.matgradient import gradcovcor, theta_to_corr, grad_corr_theta\nfrom pybhatlib.gradmvn import mvncd, mvncd_grad\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Central Finite Differences\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"\"\"\n  Forward difference:  df/dx ~ [f(x+eps) - f(x)] / eps          O(eps)\n  Central difference:  df/dx ~ [f(x+eps) - f(x-eps)] / (2*eps)  O(eps^2)\n\n  Central differences are more accurate and should always be preferred.\n\n  Choosing eps:\n  - Too large: truncation error dominates\n  - Too small: floating-point cancellation dominates\n  - Rule of thumb: eps ~ sqrt(machine_epsilon) ~ 1e-7 for float64\n\"\"\")\n\n# Demonstrate with f(x) = x^3, f'(x) = 3x^2\ndef f_cube(x):\n    return x ** 3\n\ndef grad_cube(x):\n    return 3 * x ** 2\n\nx0 = 2.0\ntrue_grad = grad_cube(x0)\n\nprint(f\"  f(x) = x^3, f'(x) = 3x^2, at x={x0}\")\nprint(f\"  True gradient: {true_grad:.6f}\\n\")\nprint(f\"  {'eps':>12s} {'forward':>12s} {'central':>12s} {'fwd_err':>12s} {'ctr_err':>12s}\")\nprint(f\"  {'-'*62}\")\n\nfor exp in range(-2, -12, -1):\n    eps = 10.0 ** exp\n    fwd = (f_cube(x0 + eps) - f_cube(x0)) / eps\n    ctr = (f_cube(x0 + eps) - f_cube(x0 - eps)) / (2 * eps)\n    print(f\"  {eps:>12.0e} {fwd:>12.6f} {ctr:>12.6f} \"\n          f\"{abs(fwd - true_grad):>12.2e} {abs(ctr - true_grad):>12.2e}\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Reusable verify_gradient() Function\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def verify_gradient(f, grad_analytic, x0, eps=1e-7, rtol=1e-4, atol=1e-6):\n    \"\"\"Verify analytic gradient against central finite differences.\n\n    Parameters\n    ----------\n    f : callable\n        Scalar function f(x) -> float.\n    grad_analytic : ndarray\n        Analytic gradient at x0.\n    x0 : ndarray\n        Point at which to verify.\n    eps : float\n        Finite difference step size.\n    rtol : float\n        Relative tolerance.\n    atol : float\n        Absolute tolerance.\n\n    Returns\n    -------\n    passed : bool\n    max_err : float\n    \"\"\"\n    n = len(x0)\n    grad_fd = np.zeros(n)\n\n    for i in range(n):\n        x_plus = x0.copy(); x_plus[i] += eps\n        x_minus = x0.copy(); x_minus[i] -= eps\n        grad_fd[i] = (f(x_plus) - f(x_minus)) / (2 * eps)\n\n    abs_err = np.abs(grad_analytic - grad_fd)\n    max_err = np.max(abs_err)\n\n    # Use relative tolerance where gradient is large, absolute where small\n    scale = np.maximum(np.abs(grad_analytic), np.abs(grad_fd))\n    scale = np.maximum(scale, 1.0)\n    rel_err = abs_err / scale\n\n    passed = np.all(rel_err < rtol) or max_err < atol\n\n    return passed, max_err, grad_fd\n\nprint(\"\"\"\n  verify_gradient(f, grad_analytic, x0) -> (passed, max_err, grad_fd)\n\n  Uses combined relative/absolute tolerance:\n  - Relative for large gradient values\n  - Absolute for near-zero gradient values\n\"\"\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Vecup — Gradient of det(matdupfull(v))\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def det_from_vec(v):\n    \"\"\"Determinant of symmetric matrix from its vecdup.\"\"\"\n    return np.linalg.det(matdupfull(v))\n\n# Analytic gradient: d(det A)/d(vec A) is cofactor matrix\nv0 = np.array([4.0, 1.0, 0.5, 3.0, 0.3, 2.0])  # vecdup of 3x3 PD matrix\nA0 = matdupfull(v0)\ndet_A0 = np.linalg.det(A0)\n\n# For vecdup gradient, we need the chain through matdupfull\n# d(det)/d(A_ij) = cofactor(i,j) = det(A) * (A^{-1})_ji\ncofactor = det_A0 * np.linalg.inv(A0).T\n# Account for symmetry: off-diagonal elements appear twice\ngrad_analytic = vecdup(cofactor)\n# Off-diagonal: multiply by 2 (since A_ij = A_ji both contribute)\nidx = 0\nK = 3\nfor i in range(K):\n    for j in range(i, K):\n        if i != j:\n            grad_analytic[idx] *= 2\n        idx += 1\n\npassed, max_err, grad_fd = verify_gradient(det_from_vec, grad_analytic, v0)\nprint(f\"\\n  f(v) = det(matdupfull(v))\")\nprint(f\"  v0 = {v0}\")\nprint(f\"  det = {det_A0:.4f}\")\nprint(f\"\\n  Analytic gradient: {grad_analytic}\")\nprint(f\"  Numerical gradient: {grad_fd}\")\nprint(f\"  Max error: {max_err:.2e}\")\nprint(f\"  Passed: {passed}\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Matgradient — Verify gradcovcor\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "omega = np.array([2.0, 1.5, 1.0])\nOmega_star = np.array([[1.0, 0.6, 0.3], [0.6, 1.0, 0.5], [0.3, 0.5, 1.0]])\nOmega = np.diag(omega) @ Omega_star @ np.diag(omega)\n\ngc = gradcovcor(Omega)\n\n# Verify: perturb omega_k, see how vecdup(Omega) changes\n# glitomega has shape (K, n_cov) = (3, 6) in BHATLIB convention\neps = 1e-7\nglitomega_fd = np.zeros_like(gc.glitomega)\nfor k in range(3):\n    om_p = omega.copy(); om_p[k] += eps\n    om_m = omega.copy(); om_m[k] -= eps\n    Om_p = np.diag(om_p) @ Omega_star @ np.diag(om_p)\n    Om_m = np.diag(om_m) @ Omega_star @ np.diag(om_m)\n    glitomega_fd[k, :] = (vecdup(Om_p) - vecdup(Om_m)) / (2 * eps)\n\nmax_err_gc = np.max(np.abs(gc.glitomega - glitomega_fd))\nprint(f\"\\n  gradcovcor.glitomega verification:\")\nprint(f\"  Max error: {max_err_gc:.2e}\")\nprint(f\"  Passed: {max_err_gc < 1e-4}\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Gradmvn — Verify mvncd_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sigma = np.array([[1.0, 0.3, 0.1], [0.3, 1.0, 0.4], [0.1, 0.4, 1.0]])\na = np.array([1.0, 0.5, 0.0])\n\nresult = mvncd_grad(a, sigma)\n\n# Verify grad_a\ndef mvncd_scalar(a_vec):\n    return mvncd(a_vec, sigma, method=\"scipy\")\n\npassed_a, max_err_a, grad_a_fd = verify_gradient(mvncd_scalar, result.grad_a, a, eps=1e-5)\n\nprint(f\"\\n  mvncd_grad.grad_a verification:\")\nprint(f\"  Analytic: {result.grad_a}\")\nprint(f\"  Numerical: {grad_a_fd}\")\nprint(f\"  Max error: {max_err_a:.2e}\")\nprint(f\"  Passed: {passed_a}\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Best Practices for Gradient Verification\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"\"\"\n  1. ALWAYS verify analytic gradients before using in optimization.\n     A small bug in the gradient can cause silent convergence failure.\n\n  2. eps selection:\n     - Default: 1e-7 for float64 (sqrt of machine epsilon)\n     - For MVNCD gradients: 1e-5 (MVNCD has inherent approximation noise)\n     - If errors are large, try both smaller and larger eps\n\n  3. Tolerance selection:\n     - Relative tolerance (rtol): 1e-4 to 1e-3\n     - Absolute tolerance (atol): 1e-6\n     - Use combined: pass if rel_err < rtol OR abs_err < atol\n\n  4. Where to check:\n     - At multiple random points (not just zeros or special cases)\n     - At the initial parameter values used for optimization\n     - At the converged solution (gradients should be ~0)\n\n  5. Common failure modes:\n     - Forgot factor of 2 for symmetric matrix off-diagonals\n     - Sign errors (maximizing vs minimizing)\n     - Missing chain rule terms\n     - Numerical noise in MVNCD approximations (use larger eps)\n\n  Template usage:\n    passed, err, grad_fd = verify_gradient(f, grad_analytic, x0)\n    assert passed, f\"Gradient check failed: max error = {err}\"\n\"\"\")\n\nprint(\"=\" * 60)\nprint(\"  Tutorial series complete!\")\nprint(\"=\" * 60)\nprint(\"\"\"\n  You've covered the full pybhatlib stack:\n\n  Level 1: vecup     — Matrix vectorization and LDLT decomposition\n  Level 2: matgradient — Covariance gradients and chain rules\n  Level 3: gradmvn   — MVNCD methods, gradients, and building blocks\n  Level 4: models/mnp — MNP estimation, control, and forecasting\n  Level 5: models/morp — MORP ordered probit\n  Level 6: advanced  — Backends, specifications, and verification\n\n  For more examples, see the examples/ directory.\n  For API reference, see the docstrings in src/pybhatlib/.\n\"\"\")\n"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}